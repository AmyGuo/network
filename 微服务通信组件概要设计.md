# 微服务通信组件概要设计

标签（空格分隔）： rpc 微服务

---

# 背景
## 通信模型
我们现有的业务由若干个服务构成。服务间的交互模式一般有以下几种：
| 模式  | 1 v 1        | 1 v N          |
| ----- | ---           | ---           |
| 同步  | 请求/响应     | --            |
| 异步  | 通知          | 发布/订阅     |
| 异步  | 请求/异步响应 | 发布/异步响应 |

其中，我们服务间的主要通讯方式rpc调用，对应的是 1 v 1同步通讯。另外，我们通过将服务分组实现了 1 v N的同步通讯。我们目前的服务间的通讯模型有如下不足：

 - 不支持异步的通讯方式，服务节点要实时在线。
 - 发布订阅基于通信的group id实现，业务上难以使用。
 - 消息没有服务质量，不能保证消息必达
   1. 消息至多传递一次(目前我们实现的消息服务质量)
   2. 消息至少传递一次
   3. 消息刚好传递一次
如果可以完善异步通信以及发布订阅，对于业务上的实现是有帮助的。类似游戏日志订阅，跨服务事务等场景都需要底层提供对应的通信模型。
   
## `dispatch`的角色

`dispatch`服务用于代理rpc客户端的请求。同时，`dispatch`服务也兼任服务注册的职责。`dispatch`主要的作用如下：

 1. 代理rpc请求，将rpc客户端的请求路由到rpc服务。由此可见，我们的服务发现机制是服务端发现，所有的rpc请求由`dispatch`代理。
 2. 代理服务注册，将所有服务按type，id以及group注册。

`dispatch`作为一个服务端发现以及服务注册组件，目前设计有如下不足：

 - 作为一个代理，内部消息转发复杂。多个`dispatch`间互连，并将代理的请求在`dispatch`间转发。可能将代理的请求直接转发给对应的服务会更合适（按服务ip和端口转发）。
 - 因为是通过代理协议通信，rpc服务端需要实现代理协议，同时增加了`dispatch`和rpc服务解析代理协议的负担。
 - 服务发现机制不够完善。`dispatch`作为一个服务端发现组件，没有提供负载均衡，按需路由等。（`dispatch`仅提供了按type和id转发的接口，需要rpc客户端实现负载均衡，hash路由等）。所以，`dispatch`代理了所有的rpc请求，但又仅提供很少部分的服务发现功能，需要在客户端也同时实现服务发现功能(rpc客户端也需要从服务注册组件获取服务信息)。感觉目前的组合是服务端发现和客户端发现的半吊子组合，职责不清晰。
 - 服务按照type，id注册弊大于利。type和id目前的主要作用是用于服务发现。本身应该只要在服务发现的组件中理解即可，业务上应该只需要指定服务名称以及路由(负载均衡)方式即可，服务发现组件负责找到对应的服务并发起调用。尤其目前，type和id的值需要在业务层指定，增加了服务注册负担。服务注册时，仅提供服务名称，服务地址（ip和端口），`dispatch`根据服务名称和路由方式找到对应的服务地址，之后发起调用会更简单。
 - 服务按group分组实现广播难以使用。将服务按type和id分组的另一个原因可能也是为了在多个服务间广播消息。这样的实现是不大合理的。首先，与type，id类似，group不应该暴露给业务层，业务层应该使用频道名称、主题等来广播，类似使用服务名称发起rpc调用一样。根据频道名称找到对应的服务实例是底层通信组件的职责；其次，rpc服务（1v1同步）和发布订阅（1vN异步）对应底层不同的通信模型，类似电话和电台广播，分开在不同的系统中实现可能会更清晰，`dispatch`作为一个代理，而不是一个消息组件，不应该且也难以实现异步发布订阅；
 - `dispatch`与rpc服务端间使用私有协议，导致rpc服务端无法使用`gRPC`等开源实现。

# 设计目标
`dispatch`作为服务端的服务发现组件以及服务注册代理，没有完整提供服务发现组件应有的负载均衡功能，并且额外承担了本应该由消息中间件承担的消息广播订阅功能。所以，本次设计的目标就是：

 1. 提供完善的底层消息通信模型
 2. 在独立的通信模型上实现rpc服务以及发布订阅
 3. 实现职责单一的客户端服务发现组件（或者服务端发现组件）
 4. 提供单独的服务注册组件
 5. 在新增的组件上实现调用链追踪等高级功能

# 设计方案

## 通信模型
### 1v1 同步通信-rpc
rpc组件主要是实现rpc请求的序列化，从网络接口发送请求，在服务端反序列化并处理，处理完毕后返回。可以自行使用protobuf等IDL定义和实现rpc服务，也可以使用gRPC等开源实现。如果没有特殊需求，完全可以使用gRPC来实现我们的服务。不管使用哪种rpc实现，都可以把rpc作为一种外围的协议包装，在内部使用同样的本地服务接口。这样，在替换rpc实现的时候，可以保持业务代码不变。
### 1vN 异步通信
之前我们缺少1vN异步通信的底层消息组件，这个也是我们难以实现1vN异步通信的关键。需要增加以下组件：
 - 消息组件。可选的有nsq，nats，redis，各种MQ（kafka等）。主要是需要支持消息持久化，以及保证消息服务质量。
 - 消息组件客户端，用于生产和消费消息。
 - 消费消息的业务服务。

目前可选的golang方案有：
1. nats-streaming支持消息持久化，以及至少一次的消息服务质量，同时nats-streaming支持高可用。
2. nsq消息队列，支持消息持久化以及至少一次的消息服务质量，nsq也支持高可用。
3. 基于redis的pub/sub实现发布订阅。


## 服务发现以及服务治理等

### 方案一
#### 客户端服务发现以及服务端服务发现
 - 客户端服务发现
   客户端通过服务注册服务拉取并监听已经注册的服务信息，并根据服务请求以及负载均衡的策略选择合适的服务发起rpc调用，并将结果返回。同时，客户端也会根据服务是否可用来屏蔽异常的服务来实现熔断和快速出错。服务限流等也可以在客户端实现。客户端服务发现的好处是客户端可以灵活地选择负载均衡策略。
```text
           request with tcp
service-A  -- -- -- -- -- -- → service-B
     |                              |
query|                              |
     ↓              register        |
service-registry ← -- -- -- -- -- ---
```
 - 服务端服务发现
   之前的dispatch类似于服务端发现。客户端将请求发送给负载均衡器，负载均衡器负载找到对应的服务实例，并发起请求，等待返回结果。这里与dispatch的区别主要在于，load-balancer将收到的请求解析出负载均衡策略后，直接透传给rpc服务。rpc服务收到的请求与上面客户端服务发现机制下收到的请求是一致的。相当于load-balancer是透明的。(原来的dispatch中，rpc服务收到的请求是包装有转发协议的请求。)
```text
           proxy request                     request with tcp
service-A  -- -- -- -- -- -- → load-balancer -- -- -- -- -- → service-B
                                 |                              |
                            query|                              |
                                 ↓              register        |
                            service-registry ← -- -- -- -- -- ---
```
建议使用客户端服务发现，主要基于以下考虑：
1. 可以根据请求端指定更加详细的负载均衡策略
2. 不需要一个高可用的load-balancer组件
3. load-balancer可能成为瓶颈
4. 目前我们只有golang和c++语言，仅开发这两个语言的客户端服务发现即可
5. 目前了解到的，例如B站的实现也是客户端服务发现

#### 服务注册中心
服务注册中心可以使用zookeeper以及etcd等实现。服务与服务注册中心保持心跳。
#### 高级功能
 - 调用链跟踪
   可以参考B站的实现,根据[Google Dapper](https://developer.ibm.com/zh/technologies/web-development/articles/wa-distributed-systems-request-tracing/)实现分布式调用链路追踪。
 - 服务度量
 - 灰度发布
 服务实行版本控制。
 - 访问控制
 - 故障恢复

### 方案二
#### Service Mesh
类似Istio等服务网格支持以下功能：
 - 负载均衡，支持http，gRPC，websocket等协议
 - 通过路由，重试，故障转移对流量进行细粒度流控
 - 通过可插拔策略层以及可配置API，能够支持流量访问控制，限速，配额管理
 - 自动度量，日志手机，调用追踪
 - 服务到服务的身份认证等
这些功能对于微服务架构来说都是很有必要的功能。而且Service Mesh以业务无关的方式实现了这些功能，与业务解耦。但是我们的服务体量太小，使用Istio可能有点杀鸡用牛刀的感觉。当然，后续可以继续调研一下，看是否适合我们使用。
另外，Service Mesh等对基于事件驱动的通信，所以对于发布订阅模式，目前暂不适用Service Mesh。

# 参考
[服务发现机制](https://blog.csdn.net/lmy86263/article/details/75156520)
[Introduction to Microservices](https://www.nginx.com/blog/introduction-to-microservices/)
[ServiceMesh究竟解决什么问题](https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651962194&idx=2&sn=7a2d8305181a394e1d01e885286a7dde&chksm=bd2d0e8e8a5a8798c17b6dcbbd0fb87ed519b685856bc480437b9ca03a665e5536d03264d91b&scene=21#wechat_redirect)
